{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "1d838125-3410-46f1-ad50-756e47aef23a",
    "_uuid": "4e67d4ef25ea55fa5bb5d3f2f25bf00de8c5ed8d"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "95e84168-16b7-4aae-b852-8c44efc18370",
    "_uuid": "df845b1c65ce102b12fb155a971482d541d7c6e9"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('input/train.csv', encoding = 'utf-8')\n",
    "test_df = pd.read_csv('input/test.csv', encoding = 'utf-8')\n",
    "#sample_submission = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# remove all non letters characters\n",
    "regex = r\"[^a-zA-Z ]\"\n",
    "# process both training and testing data\n",
    "train_processed_comments = utils.text_processing(train_df.comment_text, regex = regex)\n",
    "test_processed_comments = utils.text_processing(test_df.comment_text, regex = regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "92e85c5a-12f4-40fc-b3b2-1cb6d7f98897",
    "_uuid": "3bd9af5b0a2f750919957fb904ae19974cd8fc28"
   },
   "outputs": [],
   "source": [
    "# Get the labels of all the target columns\n",
    "target_cols = [col for col in train_df.columns if col not in ['id', 'comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the character-to-index dictionaries and caclulate the number of characters retained\n",
    "num_chars, char2idx, idx2char = utils.build_idx(train_processed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the maximum number of sentences and sentence length\n",
    "nb_sent = 4\n",
    "max_len = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to matrices\n",
    "train_X = utils.comments_to_idx(train_processed_comments, nb_sent, max_len, char2idx)\n",
    "test_X  = utils.comments_to_idx(test_processed_comments, nb_sent, max_len, char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Apply data augmentation technique with the text data. Since the model will be processing text data at the character level, three augmentation techniques were used to imitate the typos/misspelling behaviors observed in the dataset.\n",
    "\n",
    "## Three augmentation techniques\n",
    "1. Adding characters\n",
    "2. Removing characters\n",
    "3. Replace characters in the string with a random character\n",
    "\n",
    "The three augmnetation tehcniques are applied randomly to the sentences by specified probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the characters that can be used to augment the text data\n",
    "replace_set = list(set('abcdefghijklmnopqrstuvwxyz'))\n",
    "# create the augmnetation class\n",
    "aug = utils.augmentation(replace_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through each comments/sentences and augment the data\n",
    "augmented_comment = train_processed_comments.apply(lambda x: [aug.data_augmentation(sent, 0.05) for sent in x])\n",
    "# Convert the augmented data to matrix form\n",
    "augment_X = utils.comments_to_idx(augmented_comment, nb_sent, max_len, char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append augmented data to training set\n",
    "augment_train_X = np.append(train_X, augment_X, axis = 0)\n",
    "\n",
    "# Get the target data\n",
    "train_y = train_df[target_cols].as_matrix()\n",
    "\n",
    "# Double the target data to match the augmneted dataset size\n",
    "augment_train_y = np.append(train_y, train_y, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wikhu\\Anaconda3\\envs\\unsupervised-translation\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "charLSTM_models = models.charLSTM(num_outputs = len(target_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the channels, analogous to n-gram, to process the character embeddings\n",
    "channels = [(1,128), (2,128), (5,128)]\n",
    "\n",
    "# First character-LSTM model with three channels\n",
    "charLSTM1 = charLSTM_models.get_model(16, num_chars, max_len, nb_sent, channels, 0.5, highway = False)\n",
    "\n",
    "# Second character-LSTM model with Five channels\n",
    "channels = [(1,128), (2,128), (5,128), (7,128),(9,128)]\n",
    "charLSTM2 = charLSTM_models.get_model(16, num_chars, max_len, nb_sent, channels, 0.5, highway = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile parameters\n",
    "compile_params = {'loss':'binary_crossentropy', \n",
    "                  'optimizer':'adam',\n",
    "                  'metrics':['accuracy']}\n",
    "\n",
    "# Compile the models\n",
    "charLSTM1.compile(**compile_params)\n",
    "charLSTM2.compile(**compile_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1...\n",
      "Train on 287227 samples, validate on 31915 samples\n",
      "Epoch 1/2\n",
      "287227/287227 [==============================] - 204s 710us/step - loss: 0.0993 - acc: 0.9713 - val_loss: 0.0815 - val_acc: 0.9747\n",
      "Epoch 2/2\n",
      "287227/287227 [==============================] - 201s 701us/step - loss: 0.0808 - acc: 0.9752 - val_loss: 0.0777 - val_acc: 0.9758\n",
      "Fitting model 2...\n",
      "Train on 287227 samples, validate on 31915 samples\n",
      "Epoch 1/2\n",
      "287227/287227 [==============================] - 252s 877us/step - loss: 0.0919 - acc: 0.9731 - val_loss: 0.0733 - val_acc: 0.9772\n",
      "Epoch 2/2\n",
      "287227/287227 [==============================] - 248s 864us/step - loss: 0.0722 - acc: 0.9773 - val_loss: 0.0669 - val_acc: 0.9786\n"
     ]
    }
   ],
   "source": [
    "# Train the two models with different number of channels\n",
    "# Only train for two epochs because the dataset is fairly prone to overfitting\n",
    "for i, model in enumerate([charLSTM1, charLSTM2]):\n",
    "    print(\"Fitting model {}...\".format(i + 1))\n",
    "    model.fit(augment_train_X, augment_train_y, batch_size = 32, epochs = 2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the weights of the best model\n",
    "charLSTM2.save_weights('charLSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charLSTM2.load_weights('charLSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating predictions for Kaggle submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "36cec9e0-c784-497a-a839-118fa18805e3",
    "_uuid": "938174d5c26c9dc017246dbbfdd0e7124c77d612"
   },
   "outputs": [],
   "source": [
    "predict_test = charLSTM2.predict(test_X)\n",
    "\n",
    "submission = pd.DataFrame(data = predict_test)\n",
    "\n",
    "submission.columns = target_cols\n",
    "\n",
    "submission['id'] = test_df['id']\n",
    "\n",
    "submission = submission[['id'] + target_cols]\n",
    "submission.to_csv('output/submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
